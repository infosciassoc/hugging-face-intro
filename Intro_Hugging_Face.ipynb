{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "933211f0e8374961917999de3781eb7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d156c1cb289b45b4bded05cfbaa3543b",
              "IPY_MODEL_5527c20abd5e43babf104875225f07d0",
              "IPY_MODEL_8a8c7cc7fa994180a2a8a9fc3c48a721"
            ],
            "layout": "IPY_MODEL_61426768ea014320af6e6f108234b77e"
          }
        },
        "d156c1cb289b45b4bded05cfbaa3543b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_154aef4070e944aba84ec5e418a45571",
            "placeholder": "​",
            "style": "IPY_MODEL_9de916511f5d4c909a11aebab5c53f44",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5527c20abd5e43babf104875225f07d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09a6f50f5aa34cf88134ccafcb5380d0",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccdf27c3117b47e0805306c6399b0c45",
            "value": 2
          }
        },
        "8a8c7cc7fa994180a2a8a9fc3c48a721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d87290ddf1e348868e71ebbe5f8fe792",
            "placeholder": "​",
            "style": "IPY_MODEL_69d9e4cb2a1f4f4b90a23da825745ebf",
            "value": " 2/2 [00:48&lt;00:00, 23.15s/it]"
          }
        },
        "61426768ea014320af6e6f108234b77e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "154aef4070e944aba84ec5e418a45571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de916511f5d4c909a11aebab5c53f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "09a6f50f5aa34cf88134ccafcb5380d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdf27c3117b47e0805306c6399b0c45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d87290ddf1e348868e71ebbe5f8fe792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69d9e4cb2a1f4f4b90a23da825745ebf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Here we mount our google drive to this colab runtime."
      ],
      "metadata": {
        "id": "GE3Xwb8jh0Lu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FjlrCtpmusp",
        "outputId": "bb6ab774-0f20-4af3-a2eb-eeac5cab3699"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of required packages"
      ],
      "metadata": {
        "id": "bXalQzqPh7KB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qqq install huggingface_hub transformers accelerate bitsandbytes --progress-bar off"
      ],
      "metadata": {
        "id": "XJXvF--iq_Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmp0_w-vXyIq",
        "outputId": "79650ed4-e8e9-4355-9901-ea3b2eeba71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face Authentication (not mandatory but recommended)"
      ],
      "metadata": {
        "id": "Gspp7pWpiAm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "!huggingface-cli login --token $hf_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoGV29EJZH1h",
        "outputId": "1deaccd5-279e-462c-834e-dec27efc2ebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `ISA` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `ISA`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of using Pre-trained Models"
      ],
      "metadata": {
        "id": "aESM3Kn-POCU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Reduced Development Time**\n",
        "  - Pre-trained models eliminate the need to train models from scratch:\n",
        "    Training large models like GPT, BERT, or vision transformers (ViT) requires months of work and vast computational resources.\n",
        "\n",
        "    Using pre-trained models allows developers to immediately apply these state-of-the-art models to their tasks.\n",
        "\n",
        "  - Fine-tuning is faster and more cost efficient:\n",
        "    Instead of training a model from scratch, pre-trained models can be fine-tuned on specific datasets to adapt to niche use cases. This is much cheaper than training from scratch.\n",
        "\n",
        "\n",
        "2. **Access to State-of-the-Art Models**\n",
        "    - Platforms like Hugging Face provide access to a wide variety of state-of-the-art models (e.g., GPT, BERT, T5, Whisper, CLIP).\n",
        "\n",
        "   - Developers gain immediate access to cutting-edge performance for a variety of tasks:\n",
        "    - NLP Tasks: Text classification, sentiment analysis, summarization, translation, question answering, text generation.\n",
        "    - Vision Tasks: Image classification, object detection, segmentation.\n",
        "    - Multimodal Tasks: Models like CLIP handle both text and image inputs.\n",
        "    - Audio Tasks: Models like Whisper enable transcription and speech recognition.\n",
        "\n",
        "\n",
        "3. **Democratization of AI**\n",
        "- Accessible to all skill levels:\n",
        "\n",
        "    Platforms like Hugging Face simplify the deployment of advanced models, even for developers with limited AI expertise.\n",
        "    APIs and pre-built pipelines allow developers to use these models with minimal code.\n",
        "\n",
        "- Community-driven innovation:\n",
        "\n",
        "    Hugging Face's open ecosystem allows researchers and developers to share models, datasets, and ideas, accelerating progress in AI.\n",
        "\n",
        "\n",
        "4. **Improved Performance on Low-Data Tasks**\n",
        "\n",
        " - Transfer learning:\n",
        "\n",
        "    Pre-trained models excel at transferring their knowledge to new tasks, even with small datasets.\n",
        "> For example, a BERT model fine-tuned on a small dataset can outperform a model trained from scratch on the same dataset.\n",
        "\n",
        "\n",
        "5. **Proven Quality and Reliability**\n",
        "\n",
        "  - Rigorous benchmarking:\n",
        "\n",
        "    Many pre-trained models are extensively tested and benchmarked on public datasets, ensuring they meet high standards.\n",
        "\n",
        "  - Bug fixes and updates:\n",
        "    Established communities like Hugging Face ensure that popular models are frequently updated and optimized.\n",
        "\n",
        "\n",
        "6. Scalability\n",
        "  - Easily adaptable to new tasks:\n",
        "\n",
        "    By modifying the architecture slightly (e.g., adding task-specific heads to models like BERT), pre-trained models can be applied to tasks beyond their original design.\n",
        "\n",
        "  - Deployable across a range of environments:\n",
        "    Pre-trained models are compatible with various platforms, including on-premise servers, cloud systems, mobile devices, and embedded systems.\n",
        "\n",
        "\n",
        "**Challenges Addressed by Pre-Trained Models**\n",
        "  - Data Scarcity: Overcome the challenge of having limited labeled data.\n",
        "  - Lack of Expertise: Reduce the barrier to entry for advanced machine learning tasks.\n",
        "  - High Training Costs: Avoid the expense of training complex models from scratch.\n"
      ],
      "metadata": {
        "id": "j9bNCCTqjPsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We define the path of the model we will be using (copied and pasted from hugging face)"
      ],
      "metadata": {
        "id": "34855ABOiHSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"ilsp/Meltemi-7B-Instruct-v1.5\""
      ],
      "metadata": {
        "id": "4abawoqiqxj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpu = \"cuda\""
      ],
      "metadata": {
        "id": "z642L_K1ysRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #1: Sentiment Analysis, with a pretrained model."
      ],
      "metadata": {
        "id": "e33jKq0JiTLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_analyzer = pipeline(task=\"sentiment-analysis\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=\"cpu\")"
      ],
      "metadata": {
        "id": "gl1Khcm6x51d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = sentiment_analyzer(\"Hugging Face makes NLP easy and accessible!\")\n",
        "print(\"Sentiment Analysis Result 1:\", result1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Vmvz-nFtIy1",
        "outputId": "9fe73790-5a7a-48e1-f4dd-d328004c030d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Result 1: [{'label': 'POSITIVE', 'score': 0.9997339844703674}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = sentiment_analyzer(\"I hate that it is raining today ):\")\n",
        "print(\"Sentiment Analysis Result 2:\", result2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rigl9-j4tKUB",
        "outputId": "5d5650f0-2046-4381-b4e4-7d748d2e89b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Result 2: [{'label': 'NEGATIVE', 'score': 0.5724037885665894}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result3 = sentiment_analyzer(\"The service of the restaurant was great, but the food was mid.\")\n",
        "print(\"Sentiment Analysis Result 3:\", result3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dAVC4aEtNcP",
        "outputId": "a91a805c-ad7e-4100-dc6a-166771991897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Result 3: [{'label': 'NEGATIVE', 'score': 0.9593968391418457}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #2: Text Generation, with a pretrained model."
      ],
      "metadata": {
        "id": "rmENzUFQtBPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import textwrap\n",
        "\n",
        "# Set up the text wrapper\n",
        "wrapper = textwrap.TextWrapper(width=150)\n",
        "\n",
        "# Initialize the GPT-2 text generation pipeline\n",
        "generator = pipeline(task=\"text-generation\", model=\"gpt2\", device=\"cpu\")"
      ],
      "metadata": {
        "id": "d_jnQs4pOe9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator.tokenizer.eos_token_id = generator.tokenizer.pad_token_id\n",
        "# Generate the text\n",
        "generated_text = generator(\"Once upon a time in Ancient Greece,\", max_length=64, num_return_sequences=1, truncation=True)\n",
        "\n",
        "# Access the generated text\n",
        "text = generated_text[0][\"generated_text\"]\n",
        "\n",
        "# Format the text using textwrap\n",
        "formatted_generated_text = wrapper.fill(text.strip())\n",
        "\n",
        "# Print the formatted text\n",
        "print(\"\\nGenerated Text:\\n\\n\", formatted_generated_text + \"...\")"
      ],
      "metadata": {
        "id": "BhbcFA7t44Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50414886-1f30-44c7-dd60-8410e2d1a8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated Text:\n",
            "\n",
            " Once upon a time in Ancient Greece, a people that did not know what he said, called Tetrarches or \"Tetrarchs,\" an old Greek word that meant something\n",
            "like a man made a god, after he had been laid in slavery. His statue was hung in a wooden box on the side of a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Free up some Memory\n",
        "del sentiment_analyzer\n",
        "del generator"
      ],
      "metadata": {
        "id": "qv-3KBk8QVKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8-bit Quantization Overview**\n",
        "\n",
        "> Quantization is a technique to reduce the numerical precision of model weights and activations from 32-bit floating-point (FP32) or 16-bit floating-point (FP16) to 8-bit integer (INT8).\n",
        "\n",
        "This process involves:\n",
        "\n",
        "- *Scaling*: Mapping higher-precision values to a smaller 8-bit range using scaling factors.\n",
        "\n",
        "- *Calibration*: Identifying ranges (min, max) of values for weights and activations to ensure accurate representation within 8-bit limits.\n",
        "\n",
        "- *Conversion*: Replacing FP32/FP16 computations with INT8 operations during inference.\n",
        "\n",
        "\n",
        "**Pros** of 8-bit Quantization\n",
        "\n",
        "- **Reduced Memory Usage**: 8-bit weights and activations consume significantly less memory, enabling deployment on memory-constrained hardware (e.g., mobile devices).\n",
        "\n",
        "- **Faster Inference**: Operations on 8-bit integers are computationally faster than FP32/FP16, especially on hardware optimized for low-precision computations.\n",
        "\n",
        "- **Lower Power Consumption**: Reduced computational overhead leads to lower energy usage, crucial for edge devices.\n",
        "\n",
        "- **Scalability**: Allows the deployment of larger models in resource-constrained environments without prohibitive resource demands.\n",
        "\n",
        "\n",
        "**Cons** of 8-bit Quantization\n",
        "\n",
        "- **Accuracy Drop**: Quantizing from FP32/FP16 to INT8 can lead to a loss of precision, potentially reducing model accuracy.\n",
        "\n",
        "- **Numerical Instability**: For weights or activations with large dynamic ranges, 8-bit precision may struggle to represent values effectively.\n",
        "\n",
        "- **Complexity**: Additional calibration and mixed-precision handling may be required for maintaining accuracy (e.g., using FP16/FP32 for outlier values).\n",
        "\n",
        "- **Hardware Dependency**: Performance benefits depend on hardware support for INT8 operations, which may not be universal."
      ],
      "metadata": {
        "id": "h1-sJseMmMJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved model in drive has these quant config"
      ],
      "metadata": {
        "id": "jXiZf4KL5AXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Define a BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Enable 8-bit loading\n",
        "    llm_int8_threshold=5.0,  # Defines the outlier threshold for mixed-precision computation. Values above this threshold are computed in higher precision (e.g., FP16) to maintain numerical stability.\n",
        "    llm_int8_has_fp16_weight= False,  # Use both int8 and fp16 weights for mixed precision\n",
        "    llm_int8_enable_fp32_cpu_offload=False # Enables offloading specific computations to the CPU in 32-bit precision (FP32) during the 8-bit inference process\n",
        ")\n",
        "\n",
        "print(\"8-bit quantization configuration defined!\")"
      ],
      "metadata": {
        "id": "G877_ACGaTW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# Define a BitsAndBytesConfig for 8-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,  # Enable 8-bit loading\n",
        "    llm_int8_threshold=3.0,  # Defines the outlier threshold for mixed-precision computation. Values above this threshold are computed in higher precision (e.g., FP16) to maintain numerical stability.\n",
        ")\n",
        "\n",
        "print(\"8-bit quantization configuration defined!\")"
      ],
      "metadata": {
        "id": "k4A-G-ZCbTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execute this cell to load Meltemi from Hugging Face"
      ],
      "metadata": {
        "id": "3j4QgPfumgYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=gpu, quantization_config=bnb_config)\n",
        "print(\"Model loaded in 8-bit precision!\")\n",
        "print(f\"Memory used by {model_path.split('/')[1]}: {round(model.get_memory_footprint()/1024/1024/1024, 2)} GB\")"
      ],
      "metadata": {
        "id": "1a9Y9Rdnycsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run this cell if you want to save Meltemi on your Drive (7.44 GB)"
      ],
      "metadata": {
        "id": "mth2W0evmuVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# save_path = \"/content/drive/MyDrive\"\n",
        "# model.save_pretrained(os.path.join(save_path, \"Meltemi/Meltemi-7B-Instruct-v1.5_8bit_BnB_v2\"))\n",
        "# tokenizer.save_pretrained(os.path.join(save_path,\"Meltemi/Meltemi-7B-Instruct-v1.5_tokenizer_v2\"))\n",
        "\n",
        "# print(f\"Model and Tokenizer saved to {save_path}\")"
      ],
      "metadata": {
        "id": "FZFKDGo5zQxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # Load the model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/Meltemi Workshop #1/Meltemi-7B-Instruct-v1.5_8bit_BnB', device_map=gpu)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Meltemi Workshop #1/Meltemi-7B-Instruct-v1.5_tokenizer')\n",
        "\n",
        "# print(\"Model (and Tokenizer) loaded from Google Drive in 8-bit precision !\")\n",
        "# print(f\"Memory used by {model_path.split('/')[1]}: {round(model.get_memory_footprint()/1024/1024/1024, 2)} GB\")"
      ],
      "metadata": {
        "id": "CnBHamKpXeYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# # Load the model and tokenizer\n",
        "# model = AutoModelForCausalLM.from_pretrained('/content/drive/MyDrive/Meltemi Workshop #1/Meltemi-7B-Instruct-v1.5_8bit_BnB', device_map=gpu)\n",
        "# tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/Meltemi Workshop #1/Meltemi-7B-Instruct-v1.5_tokenizer')\n",
        "\n",
        "# print(\"Model (and Tokenizer) loaded from Google Drive in 8-bit precision !\")\n",
        "# print(f\"Memory used by {model_path.split('/')[1]}: {round(model.get_memory_footprint()/1024/1024/1024, 2)} GB\")"
      ],
      "metadata": {
        "id": "MsLQR9ozYeEm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "933211f0e8374961917999de3781eb7e",
            "d156c1cb289b45b4bded05cfbaa3543b",
            "5527c20abd5e43babf104875225f07d0",
            "8a8c7cc7fa994180a2a8a9fc3c48a721",
            "61426768ea014320af6e6f108234b77e",
            "154aef4070e944aba84ec5e418a45571",
            "9de916511f5d4c909a11aebab5c53f44",
            "09a6f50f5aa34cf88134ccafcb5380d0",
            "ccdf27c3117b47e0805306c6399b0c45",
            "d87290ddf1e348868e71ebbe5f8fe792",
            "69d9e4cb2a1f4f4b90a23da825745ebf"
          ]
        },
        "outputId": "15d368f6-ee09-4b29-e7e4-20d4aa092422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "933211f0e8374961917999de3781eb7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model (and Tokenizer) loaded from Google Drive in 8-bit precision !\n",
            "Memory used by Meltemi-7B-Instruct-v1.5: 7.44 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #3: Next Token Prediction, with Meltemi (GR)"
      ],
      "metadata": {
        "id": "dEv0r_V9-nDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Η τεχνητή νοημοσύνη είναι\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').to(gpu)['input_ids']\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=10,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Next-token prediction:\\n{output_text}\")"
      ],
      "metadata": {
        "id": "hAzaoKkz-pWV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95355236-5986-4f39-daef-9aecb4b50174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next-token prediction:\n",
            "Η τεχνητή νοημοσύνη είναι ένα από τα πιο σημαντικά θέματα της εποχής μας.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #4: Next Token Prediction, with Meltemi (ENG)"
      ],
      "metadata": {
        "id": "ZIp6EMcyAlTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Artificial Intelligence is\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors='pt').to(gpu)['input_ids']\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_new_tokens=50,\n",
        ")\n",
        "\n",
        "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"Next-token prediction:\\n{output_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smM8enWwAieU",
        "outputId": "fea6f8af-562d-4816-fdd7-f0873c50062b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next-token prediction:\n",
            "Artificial Intelligence is a field of computer science that aims to create intelligent machines that can perform tasks that require human intelligence, such as learning, reasoning, and perception. AI systems can be used in a variety of applications, including healthcare, finance, and education.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #5: Reasoning Problem - Chat Format, with Meltemi (GR)"
      ],
      "metadata": {
        "id": "3eU5YLRwBN4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat capability (Reasoning Problem) - Greek\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Είσαι ένας έξυπνος και ευγενικός βοηθός.\"}, # System describes the high level instructions the model should follow when generating text (referred to as preamble)\n",
        "    {\"role\": \"user\", \"content\": \"Ξεκίνησα με 13 κόκκινα μήλα και 11 πράσινα μήλα. Έδωσα 2 κόκκινα μήλα στη Μαρία. Έδωσα 3 πράσινα μήλα στον Ίππο. Μετά, έφαγα τα μισά από τα πράσινα μήλα που μου έμειναν. Αργότερα, έδωσα επίσης 2 από τα πράσινα μήλα μου στον Κώστα. Πόσα πράσινα μήλα και πόσα κόκκινα μήλα έχω τώρα;\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)\n",
        "outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nChat capability:\\n{generated_text}\")"
      ],
      "metadata": {
        "id": "BKv4Gym3tKT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de90f59-7b9d-4323-ee16-c0e186a5167d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat capability:\n",
            "\n",
            "Είσαι ένας έξυπνος και ευγενικός βοηθός. \n",
            "<|user|>\n",
            "Ξεκίνησα με 13 κόκκινα μήλα και 11 πράσινα μήλα. Έδωσα 2 κόκκινα μήλα στη Μαρία. Έδωσα 3 πράσινα μήλα στον Ίππο. Μετά, έφαγα τα μισά από τα πράσινα μήλα που μου έμειναν. Αργότερα, έδωσα επίσης 2 από τα πράσινα μήλα μου στον Κώστα. Πόσα πράσινα μήλα και πόσα κόκκινα μήλα έχω τώρα; \n",
            "<|assistant|>\n",
            "Ας το αναλύσουμε βήμα προς βήμα:\n",
            "\n",
            "1. Ξεκινήσατε με 13 κόκκινα μήλα και 11 πράσινα μήλα.\n",
            "2. Δώσατε 2 κόκκινα μήλα στη Μαρία, οπότε τώρα έχετε 13 - 2 = 11 κόκκινα μήλα.\n",
            "3. Δώσατε 3 πράσινα μήλα στον Ίππο, οπότε τώρα έχετε 11 - 3 = 8 πράσινα μήλα.\n",
            "4. Φάγατε τα μισά από τα πράσινα μήλα που σας έμειναν, οπότε τώρα έχετε 8 / 2 = 4 πράσινα μήλα.\n",
            "5. Δώσατε 2 πράσινα μήλα στον Κώστα, οπότε τώρα έχετε 4 - 2 = 2 πράσινα μήλα.\n",
            "\n",
            "Έτσι, τώρα έχετε 2 πράσινα μήλα και 11 κόκκινα μήλα.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #6: Reasoning Problem Chat Format, with Meltemi (ENG)"
      ],
      "metadata": {
        "id": "DN7pt6W3A8qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat capability (Reasoning Problem) - English\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a smart and helpful assistant.\"}, # System describes the high level instructions the model should follow when generating text\n",
        "    {\"role\": \"user\", \"content\": \"I started with 13 red apples and 11 green apples. I gave 2 red apples to Maria. I gave 3 green apples to Ippo. Then, I ate half of the green apples I had left. Later, I also gave 2 of my green apples to Kostas. How many green apples and how many red apples do I have now?\"}\n",
        "  ]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)\n",
        "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nChat capability:\\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzLuDodoC8ww",
        "outputId": "67b01e08-78dc-4274-d672-9ce858b0f146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat capability:\n",
            "\n",
            "You are a smart and helpful assistant. \n",
            "<|user|>\n",
            "I started with 13 red apples and 11 green apples. I gave 2 red apples to Maria. I gave 3 green apples to Ippo. Then, I ate half of the green apples I had left. Later, I also gave 2 of my green apples to Kostas. How many green apples and how many red apples do I have now? \n",
            "<|assistant|>\n",
            "Let's break down the problem step by step:\n",
            "\n",
            "1. You started with 13 red apples and 11 green apples.\n",
            "2. You gave 2 red apples to Maria, so you have 13 - 2 = 11 red apples left.\n",
            "3. You gave 3 green apples to Ippo, so you have 11 - 3 = 8 green apples left.\n",
            "4. You ate half of the green apples you had left, so you have 8 / 2 = 4 green apples left.\n",
            "5. You gave 2 green apples to Kostas, so you have 4 - 2 = 2 green apples left.\n",
            "\n",
            "So, you have 2 green apples and 11 red apples left.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# messages.append({\"role\": \"assistant\", \"content\": generated_text})\n",
        "# # messages.pop(-1)\n",
        "# print(f\"\\nMessages: \\n{messages[-1][-1]}\")"
      ],
      "metadata": {
        "id": "qRRbatu4IRKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #7: Reasoning Problem - Pipeline, with Meltemi (GR)"
      ],
      "metadata": {
        "id": "iV4oSrLlBcnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "# Using pipeline for chat - English\n",
        "generated_text_pipeline_chat = pipe(prompt, max_length=1024)[0]['generated_text']\n",
        "print(f\"\\nPipeline chat capability:\\n{generated_text_pipeline_chat}\")"
      ],
      "metadata": {
        "id": "-ft6ZLUFCooP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512ce1e4-236d-4bb2-aeb3-78e56d08c4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline chat capability:\n",
            "<|system|>\n",
            "Είσαι ένας έξυπνος και ευγενικός βοηθός.</s>\n",
            "<|user|>\n",
            "Ξεκίνησα με 13 κόκκινα μήλα και 11 πράσινα μήλα. Έδωσα 2 κόκκινα μήλα στη Μαρία. Έδωσα 3 πράσινα μήλα στον Ίππο. Μετά, έφαγα τα μισά από τα πράσινα μήλα που μου έμειναν. Αργότερα, έδωσα επίσης 2 από τα πράσινα μήλα μου στον Κώστα. Πόσα πράσινα μήλα και πόσα κόκκινα μήλα έχω τώρα;</s>\n",
            "<|assistant|>\n",
            "Ας το αναλύσουμε βήμα προς βήμα:\n",
            "\n",
            "1. Ξεκινήσατε με 13 κόκκινα μήλα και 11 πράσινα μήλα.\n",
            "2. Δώσατε 2 κόκκινα μήλα στη Μαρία, οπότε τώρα έχετε 13 - 2 = 11 κόκκινα μήλα.\n",
            "3. Δώσατε 3 πράσινα μήλα στον Ίππο, οπότε τώρα έχετε 11 - 3 = 8 πράσινα μήλα.\n",
            "4. Φάγατε τα μισά από τα πράσινα μήλα που σας έμειναν, οπότε τώρα έχετε 8 / 2 = 4 πράσινα μήλα.\n",
            "5. Δώσατε 2 πράσινα μήλα στον Κώστα, οπότε τώρα έχετε 4 - 2 = 2 πράσινα μήλα.\n",
            "\n",
            "Έτσι, τώρα έχετε 2 πράσινα μήλα και 11 κόκκινα μήλα.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #8: Roleplay - Chat Format, with Meltemi (GR)"
      ],
      "metadata": {
        "id": "S7bEFZ7vBkHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"Είσαι ένας Social Media Manager για έναν οργανισμό πληροφορικής, ο οποίος διοργανώνει μία ημερίδα με θέμα το πρώτο γλωσσικό μοντέλο ανοιχτού-κώδικα εξειδικευμένο στα Ελληνικά, το Μελτέμι. Στην ημερίδα θα μιλήσουν σχετικά με το έργο τους οι δημιουργοί του, ερευνητές στο Ινστιτούτο Επεξεργασίας του Λόγου, στο ερευνητικό κέντρο Αθηνά, ένα από τα μεγαλύτερα στην Ελλάδα.\"}, # System describes the high level instructions the model should follow when generating text (referred to as preamble)\n",
        "    {\"role\": \"user\", \"content\": \"Ολοκλήρωσε την πρόταση και στην συνέχεια γράψε μία μικρή παράγραφο για να ανέβει ως περιγραφή σε μία ανάρτηση στο Instagram: `Οι εξαιρετικοί αυτοί ομιλητές θα μοιραστούν μαζί μας τις γνώσεις τους και`\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)\n",
        "outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nChat capability:\\n{generated_text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNfB7b765Llc",
        "outputId": "2e8b6587-f246-4ac4-c5bf-28a06b83e989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chat capability:\n",
            "\n",
            "Είσαι ένας Social Media Manager για έναν οργανισμό πληροφορικής, ο οποίος διοργανώνει μία ημερίδα με θέμα το πρώτο γλωσσικό μοντέλο ανοιχτού-κώδικα εξειδικευμένο στα Ελληνικά, το Μελτέμι. Στην ημερίδα θα μιλήσουν σχετικά με το έργο τους οι δημιουργοί του, ερευνητές στο Ινστιτούτο Επεξεργασίας του Λόγου, στο ερευνητικό κέντρο Αθηνά, ένα από τα μεγαλύτερα στην Ελλάδα. \n",
            "<|user|>\n",
            "Ολοκλήρωσε την πρόταση και στην συνέχεια γράψε μία μικρή παράγραφο για να ανέβει ως περιγραφή σε μία ανάρτηση στο Instagram: `Οι εξαιρετικοί αυτοί ομιλητές θα μοιραστούν μαζί μας τις γνώσεις τους και` \n",
            "<|assistant|>\n",
            "Οι εξαιρετικοί αυτοί ομιλητές θα μοιραστούν μαζί μας τις γνώσεις τους και τις εμπειρίες τους σχετικά με το Μελτέμι, το πρώτο γλωσσικό μοντέλο ανοιχτού-κώδικα εξειδικευμένο στα Ελληνικά. Θα συζητήσουμε για τις δυνατότητες και τις εφαρμογές του, καθώς και για τις προκλήσεις και τις ευκαιρίες που παρουσιάζει. Επιπλέον, θα έχουμε την ευκαιρία να μάθουμε για τις τελευταίες εξελίξεις στην έρευνα και ανάπτυξη γλωσσικών μοντέλων, και να εξερευνήσουμε τις επιπτώσεις τους για το μέλλον της επεξεργασίας φυσικής γλώσσας.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example #9: Roleplay - Pipeline, with Meltemi (GR)"
      ],
      "metadata": {
        "id": "S4aicQdwBrW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "# Using pipeline for chat - Greek\n",
        "generated_text_pipeline_chat = pipe(prompt, max_length=1024)[0]['generated_text']\n",
        "print(f\"\\nPipeline chat capability:\\n{generated_text_pipeline_chat}\")"
      ],
      "metadata": {
        "id": "TVL8E_MWCsEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc7862c-eda2-4cd5-a2f4-e6a67952fa40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pipeline chat capability:\n",
            "<|system|>\n",
            "Είσαι ένας Social Media Manager για έναν οργανισμό πληροφορικής, ο οποίος διοργανώνει μία ημερίδα με θέμα το πρώτο γλωσσικό μοντέλο ανοιχτού-κώδικα εξειδικευμένο στα Ελληνικά, το Μελτέμι. Στην ημερίδα θα μιλήσουν σχετικά με το έργο τους οι δημιουργοί του, ερευνητές στο Ινστιτούτο Επεξεργασίας του Λόγου, στο ερευνητικό κέντρο Αθηνά, ένα από τα μεγαλύτερα στην Ελλάδα.</s>\n",
            "<|user|>\n",
            "Ολοκλήρωσε την πρόταση και στην συνέχεια γράψε μία μικρή παράγραφο για να ανέβει ως περιγραφή σε μία ανάρτηση στο Instagram: `Οι εξαιρετικοί αυτοί ομιλητές θα μοιραστούν μαζί μας τις γνώσεις τους και`</s>\n",
            "<|assistant|>\n",
            "Οι εξαιρετικοί αυτοί ομιλητές θα μοιραστούν μαζί μας τις γνώσεις τους και τις εμπειρίες τους σχετικά με το Μελτέμι, το πρώτο γλωσσικό μοντέλο ανοιχτού-κώδικα εξειδικευμένο στα Ελληνικά. Θα συζητήσουμε για τις δυνατότητες και τις εφαρμογές του, καθώς και για τις προκλήσεις και τις ευκαιρίες που παρουσιάζει. Επιπλέον, θα έχουμε την ευκαιρία να μάθουμε για τις τελευταίες εξελίξεις στην έρευνα και ανάπτυξη γλωσσικών μοντέλων, και να εξερευνήσουμε τις επιπτώσεις τους για το μέλλον της επεξεργασίας φυσικής γλώσσας.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examples #10:\n",
        "- Translation\n",
        "- General Knowlegde\n",
        "- Specialized Topic Knowlegde\n",
        "- Creative Writing\n",
        "- Fact Generation\n",
        "\n",
        "with Meltemi (GR & ENG)"
      ],
      "metadata": {
        "id": "uaOpX1luBxbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test cases for chat capabilities in Greek and English\n",
        "\n",
        "# English Tests\n",
        "english_test_cases = [\n",
        "    {\"role\": \"user\", \"content\": \"Translate the phrase 'Hello, how are you?' to Greek.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What is the capital of Australia?\"},\n",
        "    {\"role\": \"user\", \"content\": \"Explain quantum physics in simple terms.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a short story about a talking dog.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Give me five interesting facts about the ocean.\"},\n",
        "]\n",
        "\n",
        "# Greek Tests\n",
        "greek_test_cases = [\n",
        "    {\"role\": \"user\", \"content\": \"Μετάφρασε την φράση 'Καλημέρα, τι κάνεις;' στα αγγλικά.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Ποια είναι η πρωτεύουσα της Ελλάδας;\"},\n",
        "    {\"role\": \"user\", \"content\": \"Εξήγησε την κβαντική φυσική με απλά λόγια.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Γράψε μια μικρή ιστορία για ένα σκυλί που μιλάει.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Δώσε μου πέντε ενδιαφέροντα γεγονότα για τον ωκεανό.\"}\n",
        "]"
      ],
      "metadata": {
        "id": "3qCPny0nwGPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_test_cases(test_cases, language):\n",
        "  for test_case in test_cases:\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # Adjust system prompt if needed\n",
        "        test_case\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=512)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"{generated_text}\\n\")"
      ],
      "metadata": {
        "id": "LTJssJ0lyUn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)`\n",
        "**What this does:**\n",
        "  - `apply_chat_template` is a method on the tokenizer, possibly customized for chat-based interactions. It formats the messages input into a structured prompt that the model can understand.\n",
        "\n",
        "**Arguments:**\n",
        "  - `messages`: A list of user and assistant messages, likely in a structured format such as a list of dictionaries\n",
        "\n",
        "- `tokenize=False`: This tells the function not to tokenize the prompt yet. It means the result will still be plain text, rather than tokenized numbers.\n",
        "- `add_generation_prompt=True`: This appends the specific token \"Assistant: \" to the prompt that signals the model to continue the conversation. For example, it might append a system message like  to indicate that the next text should be generated as the assistant's response.\n",
        "\n",
        "2. `inputs = tokenizer(prompt, return_tensors=\"pt\").to(gpu)`\n",
        "**What this does:**\n",
        "- `tokenizer(prompt)`: Converts the formatted prompt from plain text into tokenized input that the model can process. Tokenization breaks the text into smaller units (e.g., words or subwords) and maps them to numeric IDs.\n",
        "\n",
        "**Arguments:**\n",
        "  - `return_tensors=\"pt\"`: Specifies that the tokenized data should be returned as PyTorch tensors, which are the data structures the model needs to process the input.\n",
        "  - `.to(gpu)`: Moves the tokenized tensors to the GPU for faster processing during model inference. We have already defined the gpu variable to be the string denoting GPU usage, `\"cuda\"`. *If you're running on a CPU, this line should be omitted or replaced with .to(\"cpu\")*.\n",
        "\n",
        "3. `outputs = model.generate(**inputs, max_new_tokens=320)`\n",
        "**What this does:**\n",
        "Calls the generate method of the model to produce text based on the tokenized inputs.\n",
        "\n",
        "  - `model.generate`: This is the main method used to generate text from a pre-trained language model. It takes the tokenized input (prepared earlier) and uses the model's architecture to predict the next tokens.\n",
        "\n",
        "  - `**inputs`: This is the unpacked dictionary of tokenized inputs (e.g., input_ids, attention_mask, etc.) that were created earlier using the tokenizer. It ensures the model has the input data needed for inference.\n",
        "\n",
        "  - `max_new_tokens=320`:\n",
        "  Limits the number of new tokens the model can generate to a maximum of 320.\n",
        "  The total length of the generated output (new tokens + input tokens) will depend on the model's max_length or context window size, but the newly added content won't exceed 320 tokens.\n",
        "  *Useful for controlling response length and avoiding overly verbose or excessively long outputs.*\n",
        "\n",
        "4. `generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)`\n",
        "**What this does:**\n",
        "Converts the generated token IDs from the model into a readable text string.\n",
        "  - `tokenizer.decode()`:\n",
        "  Converts the token IDs back into plain text by mapping each token to its corresponding word or subword. Ensures that the text is human-readable.\n",
        "\n",
        "  - `skip_special_tokens=True`:\n",
        "  Removes any special tokens (e.g., <pad>, <eos>, <bos>, etc.) from the decoded text. Ensures that the output contains only the natural language text, without extra tokens used for internal processing by the model.\n"
      ],
      "metadata": {
        "id": "B5l0zcbCeAml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running Greek tests...\")\n",
        "run_test_cases(greek_test_cases, \"Greek\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Lzg3wHySS5",
        "outputId": "78d08dfe-a666-419f-88f2-33edeab3afe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Greek tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Μετάφρασε την φράση 'Καλημέρα, τι κάνεις;' στα αγγλικά. \n",
            "<|assistant|>\n",
            "Good morning, how are you?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Ποια είναι η πρωτεύουσα της Ελλάδας; \n",
            "<|assistant|>\n",
            "Η πρωτεύουσα της Ελλάδας είναι η Αθήνα.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Εξήγησε την κβαντική φυσική με απλά λόγια. \n",
            "<|assistant|>\n",
            "Η κβαντική φυσική είναι ένας κλάδος της φυσικής που ασχολείται με τη συμπεριφορά της ύλης και της ενέργειας σε ατομικό και υποατομικό επίπεδο, όπου τα σωματίδια εμφανίζουν κυματοσωματιδιακό δυϊσμό και υπακούν σε πιθανοτικούς νόμους αντί για ντετερμινιστικούς.\n",
            "\n",
            "Σε αυτή την κλίμακα, τα σωματίδια δεν συμπεριφέρονται όπως τα κλασικά αντικείμενα, αλλά εμφανίζουν κυματοσωματιδιακό δυϊσμό, πράγμα που σημαίνει ότι έχουν τόσο κυματικές όσο και σωματιδιακές ιδιότητες. Για παράδειγμα, τα ηλεκτρόνια μπορούν να συμπεριφέρονται σαν κύματα, με το κυματάριθμό τους να καθορίζει την ενέργειά τους.\n",
            "\n",
            "Η κβαντική φυσική εισάγει επίσης την έννοια της υπέρθεσης, όπου ένα κβαντικό σύστημα μπορεί να υπάρχει σε πολλαπλές καταστάσεις ταυτόχρονα μέχρι να μετρηθεί. Για παράδειγμα, ένα ηλεκτρόνιο μπορεί να υπάρχει σε μια υπέρθεση δύο καταστάσεων μέχρι να παρατηρηθεί, οπότε θα καταρρεύσει σε μία από τις δύο καταστάσεις.\n",
            "\n",
            "Η κβαντική μηχανική, η μαθηματική περιγραφή της κβαντικής φυσικής, χρησιμοποιεί κυματοσυναρτήσεις για να περιγράψει την κατάσταση ενός κβαντικού συστήματος. Αυτές οι κυματοσυναρτήσεις παρέχουν την πιθανότητα εύρεσης του συστήματος σε μια συγκεκριμένη κατάσταση, και η μέτρηση του συστήματος \"καταρρέει\" την κυματοσυνάρτησή του σε μια συγκεκριμένη κατάσταση.\n",
            "\n",
            "Συνοψίζοντας, η κβαντική φυσική είναι μια θεμελιώδης θεωρία στη φυσική που περιγράφει τη συμπεριφορά της ύλης και της ενέργειας σε ατομικό και υποατομικό επίπεδο, όπου τα σωματίδια εμφανίζουν κυματοσωματιδιακό δυϊσμό και υπακούν σε πιθανοτικούς νόμους αντί για ντετερμινιστικούς.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Γράψε μια μικρή ιστορία για ένα σκυλί που μιλάει. \n",
            "<|assistant|>\n",
            "Μια φορά κι έναν καιρό, σε μια μικρή πόλη που ονομαζόταν Φλερτβιλ, ζούσε ένα σκυλί που ονομαζόταν Μαξ. Ο Μαξ ήταν ένα συνηθισμένο σκυλί, αλλά είχε μια ασυνήθιστη ικανότητα - μπορούσε να μιλάει. Στην αρχή, ο Μαξ δυσκολευόταν να χρησιμοποιήσει τις νέες του δεξιότητες, αλλά με τον καιρό, έγινε πιο σίγουρος για τον εαυτό του.\n",
            "\n",
            "Μια μέρα, ο Μαξ αποφάσισε να χρησιμοποιήσει τις ικανότητές του για να βοηθήσει τους φίλους του. Έμαθε ότι ένα από τα άλλα σκυλιά της πόλης, ο Ρεξ, είχε χαθεί. Ο Μαξ ήξερε ότι ο Ρεξ ήταν φοβισμένος και μπερδεμένος, οπότε αποφάσισε να τον βρει και να τον φέρει πίσω στην ασφάλεια του σπιτιού του.\n",
            "\n",
            "Ο Μαξ ξεκίνησε την αναζήτησή του, ρωτώντας τους ανθρώπους στην πόλη αν είχαν δει τον Ρεξ. Τελικά, βρήκε τον Ρεξ κρυμμένο σε ένα κοντινό πάρκο. Ο Μαξ μίλησε στον Ρεξ και τον διαβεβαίωσε ότι όλα θα πήγαιναν καλά. Μαζί, επέστρεψαν στο σπίτι του Ρεξ, όπου τον υποδέχτηκαν με ανοιχτές αγκάλες.\n",
            "\n",
            "Από εκείνη την ημέρα και μετά, ο Μαξ έγινε ήρωας στην πόλη του Φλερτβιλ. Χρησιμοποίησε τις ικανότητές του για να βοηθήσει τους φίλους του όποτε είχαν ανάγκη, και όλοι ήταν ευγνώμονες για τη βοήθειά του. Ο Μαξ έμαθε ότι το να μιλάς δεν είναι το πιο σημαντικό πράγμα στον κόσμο, αλλά μερικές φορές, μπορεί να κάνει τη διαφορά στη ζωή κάποιου.\n",
            "\n",
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Δώσε μου πέντε ενδιαφέροντα γεγονότα για τον ωκεανό. \n",
            "<|assistant|>\n",
            "1. Ο ωκεανός καλύπτει περίπου το 71% της επιφάνειας της Γης.\n",
            "2. Ο βαθύτερος γνωστός ωκεανός είναι ο Ειρηνικός Ωκεανός, με βάθος περίπου 36.000 πόδια.\n",
            "3. Ο ωκεανός περιέχει περίπου το 97% του γλυκού νερού του πλανήτη.\n",
            "4. Ο ωκεανός παράγει περίπου το 50% του οξυγόνου του πλανήτη.\n",
            "5. Ο ωκεανός είναι σπίτι για περίπου το 80% της βιοποικιλότητας του πλανήτη.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running English tests...\")\n",
        "run_test_cases(english_test_cases, \"English\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHLZudtWwZB3",
        "outputId": "085e8539-5b59-400d-a394-96b7e77d336c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running English tests...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Translate the phrase 'Hello, how are you?' to Greek. \n",
            "<|assistant|>\n",
            "Γεια σας, πώς είστε;\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "What is the capital of Australia? \n",
            "<|assistant|>\n",
            "The capital of Australia is Canberra.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Explain quantum physics in simple terms. \n",
            "<|assistant|>\n",
            "Quantum physics is a branch of physics that deals with the behavior of matter and energy at the atomic and subatomic level. It is a fundamental theory that forms the basis of modern physics and is used to explain phenomena such as the behavior of electrons, atoms, and molecules.\n",
            "\n",
            "Quantum physics is based on the concept of quantum mechanics, which states that the behavior of particles at the atomic and subatomic level is governed by probabilities rather than deterministic laws. This means that the behavior of particles at these levels is inherently uncertain and can only be described in terms of probabilities.\n",
            "\n",
            "One of the key concepts in quantum physics is the wave-particle duality, which states that particles can exhibit both wave-like and particle-like behavior. This is a fundamental principle of quantum physics and is used to explain phenomena such as the behavior of electrons in atoms and the wave-like nature of light.\n",
            "\n",
            "Another key concept in quantum physics is the principle of superposition, which states that a quantum system can exist in multiple states at the same time. This is a fundamental principle of quantum physics and is used to explain phenomena such as the behavior of quantum entanglement, where two or more particles become correlated in such a way that the quantum state of one particle is linked to the quantum state of the other, regardless of the distance between them.\n",
            "\n",
            "Quantum physics is a complex and abstract field, but its principles are used in a wide range of applications, including the development of computers and other electronic devices, the development of new materials, and the understanding of the behavior of the universe on the smallest scales.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Write a short story about a talking dog. \n",
            "<|assistant|>\n",
            "Once upon a time, in a small town called Whiskerville, there lived a dog named Max. Max was not an ordinary dog. He was a talking dog. He could communicate with humans just like they did.\n",
            "\n",
            "Max was a very curious dog. He loved to explore the town and learn about its inhabitants. He often visited the local market, where he would chat with the vendors and customers. He had a knack for solving problems, and he was always willing to help anyone in need.\n",
            "\n",
            "One day, Max overheard a group of children playing in the park. They were arguing about who should be the leader of their game. Max decided to step in and offer his help. He told the children that he could help them make a decision.\n",
            "\n",
            "The children were skeptical at first, but Max assured them that he could be a fair and impartial leader. He asked them to each make a case for why they should be the leader, and he listened carefully to each of their arguments.\n",
            "\n",
            "After hearing from all of the children, Max made his decision. He chose the child who had the most creative and innovative idea for their game. The children were surprised and delighted by Max's decision, and they thanked him for his help.\n",
            "\n",
            "From that day on, Max became a beloved figure in Whiskerville. People would often come to him for advice or help, and he was always willing to lend a paw. His talking ability made him a unique and valuable member of the community.\n",
            "\n",
            "Max's story serves as a reminder that everyone, no matter how small or insignificant they may seem, has something to offer. And sometimes, the best leaders are the ones who are willing to listen and make fair decisions.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Give me five interesting facts about the ocean. \n",
            "<|assistant|>\n",
            "1. The ocean covers approximately 71% of the Earth's surface, making it the largest body of water on our planet.\n",
            "2. The deepest point in the ocean is the Mariana Trench, located in the western Pacific Ocean. It reaches a depth of approximately 36,000 feet (11,000 meters).\n",
            "3. The ocean is home to an estimated 2 million species, with only a fraction of these having been discovered and named.\n",
            "4. The ocean plays a crucial role in regulating the Earth's climate, as it absorbs about 30% of the carbon dioxide released into the atmosphere.\n",
            "5. The ocean is a vital source of food, providing approximately 15% of the world's protein intake through fish and other seafood.\n",
            "\n",
            "\n",
            "You are a helpful assistant. \n",
            "<|user|>\n",
            "Explain what is RAG (Retrieval Augmented Generation) and why would someone use it. \n",
            "<|assistant|>\n",
            "RAG, or Retrieval Augmented Generation, is a language model that combines the capabilities of a retrieval model and a generation model. It is used to generate text that is both coherent and relevant to a given context or topic.\n",
            "\n",
            "The retrieval model in RAG is responsible for understanding the context and retrieving relevant information from a large corpus of text. This information is then used to guide the generation model, which produces text that is both coherent and relevant to the context.\n",
            "\n",
            "The main advantage of using RAG is that it can generate text that is both coherent and relevant to the context, which can be useful in a variety of applications such as chatbots, customer service, and content creation. By combining the capabilities of a retrieval model and a generation model, RAG can generate text that is both informative and engaging, which can be beneficial in a variety of contexts.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}